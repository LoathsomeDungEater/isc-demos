experiment_name = "hubert_example"
gpu_type = "24GB VRAM GPU"
nnodes = 12
venv_path = "~/.fairseq/bin/activate"
output_path = "~/outputs/hubert"
command = "fairseq_cli/hydra_train.py --config-dir examples/hubert/config/pretrain --config-name hubert_base_librispeech task.data=/mnt/.node1/Open-Datasets/librispeech/LibriSpeech/train-960/ task.label_dir=/mnt/.node1/Open-Datasets/librispeech/LibriSpeech/labels common.fp16_scale_tolerance=0.25 optimization.lr=[0.0005] task.labels='[\"km\"]' checkpoint.save_dir=$OUTPUT_PATH checkpoint.save_interval_updates=50 checkpoint.no_epoch_checkpoints=true  model.label_rate=100"
